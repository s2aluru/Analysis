{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import geocoder\n",
    "import json\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import collections\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob #For Sentiment Analysis\n",
    "import matplotlib.pyplot as plt #For Graphing the Data\n",
    "import seaborn as sns\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import re\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', -1)  # or 199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get tweets in usa about pollen allergy, get for USA only\n",
    "2. map the tweets location for positive/negative/neutral using plotly\n",
    "3. create a timeline, timeseries analysis\n",
    "4. host model on azure ml (use azure api)\n",
    "5. host project on github with real time analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'get key",
    "consumer_secret = 'get secret'\n",
    "access_token = 'get token'\n",
    "access_token_secret = 'get token secret'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret) #Fill these in\n",
    "auth.set_access_token(access_token, access_token_secret)  #Fill these in\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\spunna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word = 'allergy'\n",
    "close_words =  [search_word, 'allergies', 'amp']\n",
    "stop_words.update(close_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_latitude = 40    # geographical centre of search\n",
    "usa_longitude = -100    # geographical centre of search\n",
    "max_range = 1000           # search range in miles\n",
    "t = tw.Cursor(api.search,\n",
    "                    q='allergy -filter:retweets',\n",
    "                    tweet_mode='extended',\n",
    "                    geocode = \"%f,%f,%dkm\" % (usa_latitude, usa_longitude, max_range), \n",
    "                    lang=\"en\").items(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, special characters \n",
    "        using simple regex statements. \n",
    "        '''\n",
    "        return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", tweet).split()).lower()\n",
    "\n",
    "def clean_tweet_text(tweet): \n",
    "    return clean_tweet(tweet).lower()\n",
    "    \n",
    "def get_tweet_sentiment(tweet): \n",
    "    ''' \n",
    "    Utility function to classify sentiment of passed tweet using textblob's sentiment method \n",
    "    '''\n",
    "    # create TextBlob object of passed tweet text \n",
    "    cleaned_tweet = clean_tweet(tweet)\n",
    "    analysis = TextBlob(cleaned_tweet) \n",
    "    sentiment = 0\n",
    "    # set sentiment \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        sentiment = 1\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        sentiment = 0\n",
    "    else: \n",
    "        sentiment = -1\n",
    "    return cleaned_tweet, sentiment\n",
    "\n",
    "def get_location(location):\n",
    "    cordinates = geocoder.arcgis(location)\n",
    "    return cordinates.x, cordinates.y\n",
    "\n",
    "def get_tweets(query, count = 10): \n",
    "    # remove retweets\n",
    "    fetched_count = 0\n",
    "    new_search = query + \" -filter:retweets\"\n",
    "    tweets_columns = ['created_at', 'location', 'sentiment', 'text', 'lat', 'long']\n",
    "    tweets_df = pd.DataFrame(columns = tweets_columns)\n",
    "    try: \n",
    "        #TODO - MAXID\n",
    "        while (fetched_count < count):\n",
    "            fetched_tweets = tw.Cursor(api.search,\n",
    "                       q=new_search,\n",
    "                       geocode = \"%f,%f,%dmi\" % (usa_latitude, usa_longitude, max_range), \n",
    "                       tweet_mode='extended',\n",
    "                       lang=\"en\").items(count)\n",
    "            \n",
    "            for tweet in fetched_tweets:\n",
    "                if (tweet.user.location != ''):\n",
    "                    text, sentiment = get_tweet_sentiment(tweet.full_text)\n",
    "                    long, lat = get_location(tweet.user.location)\n",
    "                    tweets_df = tweets_df.append({'created_at':tweet.created_at, \n",
    "                                      'location':tweet.user.location,\n",
    "                                      'sentiment':sentiment,\n",
    "                                      'text':text,\n",
    "                                      'lat':lat,\n",
    "                                      'long':long},\n",
    "                                     ignore_index=True)\n",
    "                    fetched_count = fetched_count + 1\n",
    "            print(fetched_count, tweets_df.shape, sentiment)\n",
    "                 \n",
    "       # tweets_df = fetched_count, pd.DataFrame(data = tweets, columns=tweets_columns)            \n",
    "        return fetched_count, tweets_df\n",
    "    except tweepy.TweepError as e: \n",
    "        print(\"Error : \" + str(e)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 (192, 6) 1\n",
      "384 (384, 6) 1\n",
      "576 (576, 6) 1\n",
      "768 (768, 6) 1\n",
      "960 (960, 6) 1\n",
      "1152 (1152, 6) 1\n",
      "Total tweets fetched for allergy: 1152\n",
      "Wall time: 11min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fetched_count, tweets = get_tweets(query = '#allergy', count = 1000) \n",
    "if  ( fetched_count > 0):\n",
    "    print('Total tweets fetched for allergy:', fetched_count)\n",
    "    allergyTweets = tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allergyTweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Words Found in Tweets (Without Stop or Collection Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check # of positive, negative and neutral tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show();\n",
    "## declare the variables for the pie chart, using the Counter variables for “sizes”\n",
    "sentiments = pd.Series(data=allergyTweets['sentiment'].value_counts(), index=[-1,0,1])\n",
    "labels = 'Positive', 'Negative', 'Neutral'\n",
    "sizes = [sentiments[1], sentiments[-1], sentiments[0]]\n",
    "colors = ['green', 'red', 'yellow']\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',shadow=False, startangle=90)\n",
    "ax1.axis('equal')\n",
    "plt.title('Sentiment of {} tweets about allergy'.format(fetched_count))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_words = ' '.join(allergyTweets.text).split()\n",
    "tweets_unsw = [[word for word in tweet_words if not word in stop_words]][0]\n",
    "counts_nsw_nc= collections.Counter(tweets_unsw)\n",
    "clean_tweets_ncw = pd.DataFrame(counts_nsw_nc.most_common(15),\n",
    "                             columns=['words', 'count'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "# Plot horizontal bar graph\n",
    "clean_tweets_ncw.sort_values(by='count').plot.barh(x='words',\n",
    "                      y='count',\n",
    "                      ax=ax,\n",
    "                      color=\"blue\")\n",
    "ax.set_title(\"Common words found in tweets out of total word count: {}\".format(len(tweets_unsw)))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_bigram = list(bigrams(tweets_unsw))\n",
    "bigram_counts = collections.Counter(terms_bigram)\n",
    "bigram_df = pd.DataFrame(bigram_counts.most_common(20),\n",
    "                             columns=['bigram', 'count'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing network of bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of bigrams and their counts\n",
    "d = bigram_df.set_index('bigram').T.to_dict('records')\n",
    "# Create network plot \n",
    "G = nx.Graph()\n",
    "\n",
    "# Create connections between nodes\n",
    "for k, v in d[0].items():\n",
    "    G.add_edge(k[0], k[1], weight=(v * 10))\n",
    "\n",
    "G.add_node(\"allergy\", weight=100)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "pos = nx.spring_layout(G, k=1)\n",
    "\n",
    "# Plot networks\n",
    "nx.draw_networkx(G, pos,\n",
    "                 font_size=16,\n",
    "                 width=3,\n",
    "                 edge_color='grey',\n",
    "                 node_color='purple',\n",
    "                 with_labels = False,\n",
    "                 ax=ax)\n",
    "\n",
    "# Create offset labels\n",
    "for key, value in pos.items():\n",
    "    x, y = value[0]+.135, value[1]+.045\n",
    "    ax.text(x, y,\n",
    "            s=key,\n",
    "            bbox=dict(facecolor='red', alpha=0.25),\n",
    "            horizontalalignment='center', fontsize=13)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetColor(x):\n",
    "    if(x == 1):\n",
    "        return \"green\"\n",
    "    elif(x == 0):\n",
    "        return \"yellow\"\n",
    "    else:\n",
    "        return \"red\"\n",
    "    \n",
    "data = [ go.Scattergeo(\n",
    "        locationmode = 'USA-states',\n",
    "        lon = allergyTweets['long'],\n",
    "        lat = allergyTweets['lat'],\n",
    "        text = allergyTweets['text'],\n",
    "        mode = 'markers',\n",
    "        marker = dict( \n",
    "            size = 8, \n",
    "            opacity = 0.8,\n",
    "            symbol = 'circle',\n",
    "            line = dict(\n",
    "                width=1,\n",
    "                color='rgba(102, 102, 102)'\n",
    "            ),\n",
    "            cmin = 0,\n",
    "            color=list(map(SetColor, allergyTweets['sentiment']))\n",
    "           \n",
    "        ))]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Location of Twitter users who talked about allergy', \n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),\n",
    "            showland = True,\n",
    "            landcolor = \"rgb(250, 250, 250)\",\n",
    "            subunitcolor = \"rgb(217, 217, 217)\",\n",
    "            countrycolor = \"rgb(217, 217, 217)\",\n",
    "            countrywidth = 0.5,\n",
    "            subunitwidth = 0.5        \n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout )\n",
    "py.iplot(fig, filename='usa-twitter-allergy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'urllib' has no attribute 'quote'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-67f4752e26ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mmerged_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mmerged_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0msorted_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0msignature_base_str\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmethod\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconcat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconcat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-67f4752e26ac>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mmerged_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mmerged_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0msorted_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0msignature_base_str\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmethod\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconcat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconcat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'urllib' has no attribute 'quote'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Weather API Python sample code\n",
    "\n",
    "Copyright 2019 Oath Inc. Licensed under the terms of the zLib license see https://opensource.org/licenses/Zlib for terms.\n",
    "\n",
    "$ python --version\n",
    "Python 2.7.10\n",
    "\n",
    "\"\"\"\n",
    "APP_ID=\"vWaHUN5c\"\n",
    "apikey=\"dj0yJmk9TTFjR0JXcEhCYmF4JnM9Y29uc3VtZXJzZWNyZXQmc3Y9MCZ4PWE0\"\n",
    "apisecret=\"d713f97b5ec5f2b4430cda140088f5a4e9cfde17\"\n",
    "\n",
    "import time, uuid, urllib, urllib3\n",
    "import hmac, hashlib\n",
    "from base64 import b64encode\n",
    "\n",
    "\"\"\"\n",
    "Basic info\n",
    "\"\"\"\n",
    "url = 'https://weather-ydn-yql.media.yahoo.com/forecastrss'\n",
    "method = 'GET'\n",
    "app_id = APP_ID\n",
    "consumer_key = apikey\n",
    "consumer_secret = apisecret\n",
    "concat = '&'\n",
    "query = {'location': 'sunnyvale,ca', 'format': 'json'}\n",
    "oauth = {\n",
    "    'oauth_consumer_key': consumer_key,\n",
    "    'oauth_nonce': uuid.uuid4().hex,\n",
    "    'oauth_signature_method': 'HMAC-SHA1',\n",
    "    'oauth_timestamp': str(int(time.time())),\n",
    "    'oauth_version': '1.0'\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Prepare signature string (merge all params and SORT them)\n",
    "\"\"\"\n",
    "merged_params = query.copy()\n",
    "merged_params.update(oauth)\n",
    "sorted_params = [k + '=' + urllib.quote(merged_params[k], safe='') for k in sorted(merged_params.keys())]\n",
    "signature_base_str =  method + concat + urllib.quote(url, safe='') + concat + urllib.quote(concat.join(sorted_params), safe='')\n",
    "\n",
    "\"\"\"\n",
    "Generate signature\n",
    "\"\"\"\n",
    "composite_key = urllib.quote(consumer_secret, safe='') + concat\n",
    "oauth_signature = b64encode(hmac.new(composite_key, signature_base_str, hashlib.sha1).digest())\n",
    "\n",
    "\"\"\"\n",
    "Prepare Authorization header\n",
    "\"\"\"\n",
    "oauth['oauth_signature'] = oauth_signature\n",
    "auth_header = 'OAuth ' + ', '.join(['{}=\"{}\"'.format(k,v) for k,v in oauth.iteritems()])\n",
    "\n",
    "\"\"\"\n",
    "Send request\n",
    "\"\"\"\n",
    "url = url + '?' + urllib.urlencode(query)\n",
    "request = urllib3.Request(url)\n",
    "request.add_header('Authorization', auth_header)\n",
    "request.add_header('X-Yahoo-App-Id', app_id)\n",
    "response = urllib3.urlopen(request).read()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests_oauthlib in c:\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from requests_oauthlib) (2.21.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\anaconda3\\lib\\site-packages (from requests_oauthlib) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests_oauthlib) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests_oauthlib) (3.0.3)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests_oauthlib) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests_oauthlib) (2.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests_oauthlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from requests_oauthlib import OAuth1Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, uuid, urllib\n",
    "from json import loads\n",
    "import hmac, hashlib\n",
    "from base64 import b64encode\n",
    "\n",
    "\n",
    "def _generate_signature(key, data):\n",
    "    key_bytes= bytes(key , 'utf-8')\n",
    "    data_bytes = bytes(data, 'utf-8') \n",
    "    signature =  hmac.new(\n",
    "        key_bytes,\n",
    "        data_bytes,\n",
    "        hashlib.sha1\n",
    "    ).digest()\n",
    "    return b64encode(signature).decode()\n",
    "\n",
    "\n",
    "\n",
    "def get_yahoo_weather(\n",
    "    location,\n",
    "    url='https://weather-ydn-yql.media.yahoo.com/forecastrss'\n",
    "):\n",
    "    app_id = APP_ID\n",
    "    consumer_key = apikey\n",
    "    consumer_secret = apisecret\n",
    "    # Basic info\n",
    "    method = 'GET'\n",
    "    concat = '&'\n",
    "    query = {\n",
    "        'location': location,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    oauth = {\n",
    "        'oauth_consumer_key': consumer_key,\n",
    "        'oauth_nonce': uuid.uuid4().hex,\n",
    "        'oauth_signature_method': 'HMAC-SHA1',\n",
    "        'oauth_timestamp': str(int(time.time())),\n",
    "        'oauth_version': '1.0'\n",
    "    }\n",
    "\n",
    "    # Prepare signature string (merge all params and SORT them)\n",
    "    merged_params = query.copy()\n",
    "    merged_params.update(oauth)\n",
    "    sorted_params = [\n",
    "        k + '=' + urllib.parse.quote(merged_params[k], safe='')\n",
    "        for k in sorted(merged_params.keys())\n",
    "    ]\n",
    "    signature_base_str = (\n",
    "        method + \n",
    "        concat + \n",
    "        urllib.parse.quote(\n",
    "            url,\n",
    "            safe=''\n",
    "        ) +\n",
    "        concat + \n",
    "        urllib.parse.quote(concat.join(sorted_params), safe='')\n",
    "    )\n",
    "\n",
    "    # Generate signature\n",
    "    composite_key = urllib.parse.quote(\n",
    "        consumer_secret,\n",
    "        safe=''\n",
    "    ) + concat\n",
    "    oauth_signature = _generate_signature(\n",
    "        composite_key,\n",
    "        signature_base_str\n",
    "    )\n",
    "\n",
    "    # Prepare Authorization header\n",
    "    oauth['oauth_signature'] = oauth_signature\n",
    "    auth_header = (\n",
    "        'OAuth ' + \n",
    "        ', '.join(\n",
    "            [\n",
    "                '{}=\"{}\"'.format(k,v) \n",
    "                for k,v in oauth.items()\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Send request\n",
    "    url = url + '?' + urllib.parse.urlencode(query)\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header('Authorization', auth_header)\n",
    "    request.add_header('X-Yahoo-App-Id', app_id)\n",
    "    response = urllib.request.urlopen(request).read()\n",
    "    return loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_observation': {'astronomy': {'sunrise': '7:13 am',\n",
       "   'sunset': '7:30 pm'},\n",
       "  'atmosphere': {'humidity': 85,\n",
       "   'pressure': 29.94,\n",
       "   'rising': 0,\n",
       "   'visibility': 10.0},\n",
       "  'condition': {'code': 30, 'temperature': 34, 'text': 'Partly Cloudy'},\n",
       "  'pubDate': 1553428800,\n",
       "  'wind': {'chill': 34, 'direction': 0, 'speed': 0.62}},\n",
       " 'forecasts': [{'code': 30,\n",
       "   'date': 1553400000,\n",
       "   'day': 'Sun',\n",
       "   'high': 69,\n",
       "   'low': 33,\n",
       "   'text': 'Partly Cloudy'},\n",
       "  {'code': 11,\n",
       "   'date': 1553486400,\n",
       "   'day': 'Mon',\n",
       "   'high': 69,\n",
       "   'low': 52,\n",
       "   'text': 'Showers'},\n",
       "  {'code': 11,\n",
       "   'date': 1553572800,\n",
       "   'day': 'Tue',\n",
       "   'high': 55,\n",
       "   'low': 40,\n",
       "   'text': 'Showers'},\n",
       "  {'code': 34,\n",
       "   'date': 1553659200,\n",
       "   'day': 'Wed',\n",
       "   'high': 53,\n",
       "   'low': 35,\n",
       "   'text': 'Mostly Sunny'},\n",
       "  {'code': 32,\n",
       "   'date': 1553745600,\n",
       "   'day': 'Thu',\n",
       "   'high': 61,\n",
       "   'low': 33,\n",
       "   'text': 'Sunny'},\n",
       "  {'code': 34,\n",
       "   'date': 1553832000,\n",
       "   'day': 'Fri',\n",
       "   'high': 70,\n",
       "   'low': 42,\n",
       "   'text': 'Mostly Sunny'},\n",
       "  {'code': 30,\n",
       "   'date': 1553918400,\n",
       "   'day': 'Sat',\n",
       "   'high': 73,\n",
       "   'low': 49,\n",
       "   'text': 'Partly Cloudy'},\n",
       "  {'code': 39,\n",
       "   'date': 1554004800,\n",
       "   'day': 'Sun',\n",
       "   'high': 68,\n",
       "   'low': 52,\n",
       "   'text': 'Scattered Showers'},\n",
       "  {'code': 30,\n",
       "   'date': 1554091200,\n",
       "   'day': 'Mon',\n",
       "   'high': 61,\n",
       "   'low': 47,\n",
       "   'text': 'Partly Cloudy'},\n",
       "  {'code': 30,\n",
       "   'date': 1554177600,\n",
       "   'day': 'Tue',\n",
       "   'high': 65,\n",
       "   'low': 46,\n",
       "   'text': 'Partly Cloudy'}],\n",
       " 'location': {'city': 'Raleigh',\n",
       "  'country': 'United States',\n",
       "  'lat': 35.785511,\n",
       "  'long': -78.64267,\n",
       "  'region': ' NC',\n",
       "  'timezone_id': 'America/New_York',\n",
       "  'woeid': 2478307}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_yahoo_weather('Raleigh, NC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
